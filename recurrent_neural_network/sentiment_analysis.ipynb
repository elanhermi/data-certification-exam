{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Sentiment Analysis of French Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDsXJRI3Wfrv"
   },
   "source": [
    "### Objectives\n",
    "1. Text cleaning\n",
    "2. Text preprocessing for custom embedding Neural Network\n",
    "3. Train RNN model for sentiment analysis\n",
    "\n",
    "‚ö†Ô∏è This notebook will be your final deliverable. \n",
    "- Make sure it can run \"restart and run all\"\n",
    "- Delete useless code cells\n",
    "- Do not \"clear output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvwURl10Wmw1"
   },
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 30,000 french reviews of movies, along with the binary class 1 (positive) or 0 (negative) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/elan/.pyenv/versions/3.8.6/envs/backinthessr/lib/python3.8/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/elan/.pyenv/versions/3.8.6/envs/backinthessr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "!pip install unidecode\n",
    "import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1615382505157,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "IufC0UUhxyGC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>√áa commence √† devenir √©nervant d'avoir l'impre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J'ai aim√© ce film, si il ressemble a un docume...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Une grosse merde ce haneke ce faire produire p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beau m√©lodrame magnifiquement photographi√©, \"V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A la poursuite du diamant vers est un film pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29946</th>\n",
       "      <td>Le meilleur film de super-h√©ros derri√®re le ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29947</th>\n",
       "      <td>Un drame qui est d'une efficacit√© remarquable....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29948</th>\n",
       "      <td>Une daube hollywoodienne de plus, aucun int√©r√™...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29949</th>\n",
       "      <td>Et voil√† un nouveau biopic sur la star du X Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29950</th>\n",
       "      <td>Un film qui fait vieux, avec des acteurs pas t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29951 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  polarity\n",
       "0      √áa commence √† devenir √©nervant d'avoir l'impre...         0\n",
       "1      J'ai aim√© ce film, si il ressemble a un docume...         1\n",
       "2      Une grosse merde ce haneke ce faire produire p...         0\n",
       "3      Beau m√©lodrame magnifiquement photographi√©, \"V...         1\n",
       "4      A la poursuite du diamant vers est un film pro...         1\n",
       "...                                                  ...       ...\n",
       "29946  Le meilleur film de super-h√©ros derri√®re le ba...         1\n",
       "29947  Un drame qui est d'une efficacit√© remarquable....         1\n",
       "29948  Une daube hollywoodienne de plus, aucun int√©r√™...         0\n",
       "29949  Et voil√† un nouveau biopic sur la star du X Li...         0\n",
       "29950  Un film qui fait vieux, avec des acteurs pas t...         0\n",
       "\n",
       "[29951 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the dataset for you\n",
    "data = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1615382505157,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "IufC0UUhxyGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    15051\n",
      "0    14900\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We create features\n",
    "y = data.polarity\n",
    "X = data.review\n",
    "\n",
    "# We analyse class balance\n",
    "print(pd.value_counts(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1615383356787,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "yzIpNmSg0XV4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity: 0 \n",
      "\n",
      "√áa commence √† devenir √©nervant d'avoir l'impression de voir et revoir le m√™me genre de film √† savoir : la com√©die romantique, surement le genre le plus prolifique de le production fran√ßaise actuelle. Le probl√®me c'est que l'on a souvent affaire √† des niaiseries de faible niveau comme celui ci. Avec un scenario ultra balis√© et conventionnel, c'est √† se demander comment √ßa peut passer les portes d'un producteur. Bref cette sempiternel histoire d'un homme mentant au nom de l'amour pour reconqu√©rir une femme et qui √† la fin se prend son mensonge en pleine figure est d'une originalit√© affligeante, et ce n'est pas la pr√©sence au casting de l'ex miss m√©t√©o Charlotte Le Bon qui r√™ve surement d'avoir la m√™me carri√®re que Louise Bourgoin qui change la donne.\n"
     ]
    }
   ],
   "source": [
    "# We check various reviews\n",
    "print(f'polarity: {y[0]} \\n')\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We need to give a _quick & dirty_ cleaning to all the sentences in the dataset. Create a variable `X_clean` of similar shape, but with the following cleaning:\n",
    "- Replace french accents by their non-accentuated equivalent using the [unidecode.unidecode()](https://pypi.org/project/Unidecode/) method\n",
    "- Reduce all uppercases to lowercases\n",
    "- Remove any characters outside of a-z, for instance using `string.isalpha()`\n",
    "\n",
    "üòå You will be given the solution `X_clean` in the next question to make sure you can complete the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing accents\n",
    "def remove_accents(text):\n",
    "    no_accent = unidecode.unidecode(text)\n",
    "    return no_accent\n",
    "\n",
    "data['clean'] = data['review'].apply(lambda text: remove_accents(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing \n",
    "def lowercase(text): \n",
    "    lowercased = text.lower() \n",
    "    return lowercased\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda text: lowercase(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(text):\n",
    "    text = ''.join(c for c in text if c.isalpha() or c == ' ')\n",
    "    return re.sub(\" +\", \" \", text)\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda text: remove_chars(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = data['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('C14',\n",
    "    shape = X_clean.shape,\n",
    "    first_sentence = X_clean[0]\n",
    ")\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clean sentences, we need to convert each one into a list of integers of fixed size\n",
    "- For example, the sentence: `\"this was good\"` should become something like `array([1, 3, 18, 0, 0, 0, ...0], dtype=int32)` where each integer match to a each _unique_ word in your corpus of sentences.\n",
    "\n",
    "‚ùì Create a numpy ndarray `X_input` of shape (29951, 100) that will be the direct input to your Neutral Network. \n",
    "\n",
    "- 29951 represents the number of reviews in the dataset `X_clean`\n",
    "- 100 represents the maximum number of words to keep for each movie review.\n",
    "- It must contain only numerical values, without any `NaN`\n",
    "- In the process, compute and save the number of _unique_ words in your cleaned corpus under `vocab_size` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First, you **must** start back from the clean solution below (14Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ca commence a devenir enervant de voir et revo...\n",
       "1        aime ce film si il ressemble a un documentaire...\n",
       "2        une grosse merde ce haneke ce faire produire p...\n",
       "3        beau melodrame magnifiquement photographie ver...\n",
       "4        a la poursuite du diamant vers est un film pro...\n",
       "                               ...                        \n",
       "29946    le meilleur film de derriere le batman de nola...\n",
       "29947    un drame qui est efficacite remarquable un fil...\n",
       "29948    une daube hollywoodienne de plus aucun interet...\n",
       "29949    et voila un nouveau biopic sur la star du x li...\n",
       "29950    un film qui fait vieux avec des acteurs pas to...\n",
       "Name: review, Length: 29951, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies_X_clean.csv\")['review']\n",
    "X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def convert_sentences(X):\n",
    "    return [sentence.split(' ') for sentence in X]\n",
    "\n",
    "def tokenize(sentences, word_to_id):\n",
    "    return [[word_to_id[_] for _ in s if _ in word_to_id] for s in sentences]\n",
    "\n",
    "arr = X_clean.to_numpy()\n",
    "X_clean = convert_sentences(arr)\n",
    "\n",
    "word_to_id = {}\n",
    "iter_ = 1\n",
    "for sentence in X_clean:\n",
    "    for word in sentence:\n",
    "        if word in word_to_id:\n",
    "            continue\n",
    "        word_to_id[word] = iter_\n",
    "        iter_ += 1\n",
    "vocab_size = len(word_to_id)\n",
    "X_token = tokenize(X_input, word_to_id)\n",
    "X_pad = pad_sequences(X_token, dtype='float32', padding='post', value=0, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29951, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('C1415',\n",
    "    type_X = type(X_input),\n",
    "    shape = X_input.shape, \n",
    "    input_1 = X_input[1], \n",
    ")\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjM5UP5ZMbY_"
   },
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCreate and fit a Neural Netork that takes `X_input` and `y` as input, to binary classify each sentence's sentiment\n",
    "\n",
    "- You cannot use transfer learning or other pre-existing Word2Vec models\n",
    "- You must use a \"recurrent\" architecture to _capture_ a notion of order in the sentences' words\n",
    "- The performance metrics for this task is \"accuracy\"\n",
    "- Store your model in a variable `model` \n",
    "- Store the result your `model.fit()` in a variable `history`. \n",
    "- ‚ö†Ô∏è `history.history` must comprises a measure of the `val_accuracy` at each epoch.\n",
    "- You don't need to cross-validate your model\n",
    "\n",
    "üòå Don't worry, you will not be judged on your computer power: You should be able to reach accuracy significantly better than baseline in less than 3 minutes even without GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ But first, you **must** start back from the solution below (70Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies_X_input.csv'\n",
    "X_input = np.genfromtxt(url, delimiter=',', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y, test_size=0.3)\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size +1, output_dim=10, mask_zero=True, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=4, activation='relu', padding='same'))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy', \n",
    "    metrics = 'accuracy')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "459/459 [==============================] - 14s 29ms/step - loss: 0.6147 - accuracy: 0.6081 - val_loss: 0.2608 - val_accuracy: 0.8952\n",
      "Epoch 2/50\n",
      "459/459 [==============================] - 16s 34ms/step - loss: 0.1793 - accuracy: 0.9328 - val_loss: 0.2522 - val_accuracy: 0.8998\n",
      "Epoch 3/50\n",
      "459/459 [==============================] - 20s 44ms/step - loss: 0.0766 - accuracy: 0.9736 - val_loss: 0.3028 - val_accuracy: 0.8943\n",
      "Epoch 4/50\n",
      "459/459 [==============================] - 10s 22ms/step - loss: 0.0432 - accuracy: 0.9831 - val_loss: 0.4138 - val_accuracy: 0.8909\n",
      "Epoch 5/50\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 0.0275 - accuracy: 0.9908 - val_loss: 0.4609 - val_accuracy: 0.8865\n",
      "Epoch 6/50\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 0.0232 - accuracy: 0.9919 - val_loss: 0.5391 - val_accuracy: 0.8801\n",
      "Epoch 7/50\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.5534 - val_accuracy: 0.8820\n",
      "Epoch 8/50\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 0.0145 - accuracy: 0.9945 - val_loss: 0.6360 - val_accuracy: 0.8798\n",
      "Epoch 9/50\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.6168 - val_accuracy: 0.8827\n",
      "Epoch 10/50\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.6902 - val_accuracy: 0.8790\n",
      "Epoch 11/50\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.7368 - val_accuracy: 0.8779\n",
      "Epoch 12/50\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.7180 - val_accuracy: 0.8741\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.3, epochs=50, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4848234951496124,\n",
       "  0.18120992183685303,\n",
       "  0.0831550657749176,\n",
       "  0.04385509714484215,\n",
       "  0.031990498304367065,\n",
       "  0.023349978029727936,\n",
       "  0.017903516069054604,\n",
       "  0.01511790044605732,\n",
       "  0.014441867358982563,\n",
       "  0.01150391437113285,\n",
       "  0.011629046872258186,\n",
       "  0.009965200908482075],\n",
       " 'accuracy': [0.7323338985443115,\n",
       "  0.9304258823394775,\n",
       "  0.9702895879745483,\n",
       "  0.984190821647644,\n",
       "  0.9894378185272217,\n",
       "  0.991413950920105,\n",
       "  0.9942078590393066,\n",
       "  0.9945485591888428,\n",
       "  0.995570719242096,\n",
       "  0.9965246915817261,\n",
       "  0.9961839914321899,\n",
       "  0.9970698356628418],\n",
       " 'val_loss': [0.2607869505882263,\n",
       "  0.2521746754646301,\n",
       "  0.302751362323761,\n",
       "  0.4138467609882355,\n",
       "  0.46090567111968994,\n",
       "  0.5390968918800354,\n",
       "  0.5533552765846252,\n",
       "  0.6360172629356384,\n",
       "  0.6168342232704163,\n",
       "  0.6901570558547974,\n",
       "  0.7367926239967346,\n",
       "  0.7180129289627075],\n",
       " 'val_accuracy': [0.8952305316925049,\n",
       "  0.8998410105705261,\n",
       "  0.8942766189575195,\n",
       "  0.8909379839897156,\n",
       "  0.8864864706993103,\n",
       "  0.8801271915435791,\n",
       "  0.882034957408905,\n",
       "  0.8798092007637024,\n",
       "  0.8826708793640137,\n",
       "  0.8790143132209778,\n",
       "  0.8779014348983765,\n",
       "  0.8740858435630798]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('C1517',\n",
    "                         history=history.history)\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNt966tqZXM2p288pQsUAUV",
   "name": "certification_DL_NLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
